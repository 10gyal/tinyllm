{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # Define a regular expression pattern to match mentions\n",
    "    mention_pattern = r'@\\w+'\n",
    "    # Define regular expression patterns for emojis and links\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # Alphabetic presentation forms\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric shapes\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Miscellaneous symbols\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols and pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Extended-A\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Extended-B\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    link_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n",
    "                              flags=re.UNICODE)\n",
    "    \n",
    "    # Remove emojis and links from the tweet\n",
    "    tweet = emoji_pattern.sub('', tweet)\n",
    "    tweet = link_pattern.sub('', tweet)\n",
    "    \n",
    "\n",
    "    # Use re.sub to replace mentions with an empty string\n",
    "    clean_t = re.sub(mention_pattern, '', tweet)\n",
    "    \n",
    "    return clean_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: clean_tweet(x).strip().replace('&amp;', \"and\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['text'].apply(lambda x: x + '.' if x[-1]!='.' else x).str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character level tokenization is sufficient for a small dataset. First some stats for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 293038\n",
      "Vocab size: 101\n",
      "Vocab content: ['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\xa0', 'à', 'é', 'ō', '\\u200d', '–', '‘', '’', '“', '”', '…', '≠']\n"
     ]
    }
   ],
   "source": [
    "corpus_len = len(corpus)\n",
    "print(\"Corpus length:\", corpus_len)\n",
    "vocab = sorted(list(set(corpus)))\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Vocab content:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a char:index mapping where the index will serve as the index for the token embedding of the char it maps to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char to idx: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, ';': 28, '?': 29, 'A': 30, 'B': 31, 'C': 32, 'D': 33, 'E': 34, 'F': 35, 'G': 36, 'H': 37, 'I': 38, 'J': 39, 'K': 40, 'L': 41, 'M': 42, 'N': 43, 'O': 44, 'P': 45, 'Q': 46, 'R': 47, 'S': 48, 'T': 49, 'U': 50, 'V': 51, 'W': 52, 'X': 53, 'Y': 54, 'Z': 55, '[': 56, ']': 57, '_': 58, 'a': 59, 'b': 60, 'c': 61, 'd': 62, 'e': 63, 'f': 64, 'g': 65, 'h': 66, 'i': 67, 'j': 68, 'k': 69, 'l': 70, 'm': 71, 'n': 72, 'o': 73, 'p': 74, 'q': 75, 'r': 76, 's': 77, 't': 78, 'u': 79, 'v': 80, 'w': 81, 'x': 82, 'y': 83, 'z': 84, '{': 85, '|': 86, '}': 87, '~': 88, '\\xa0': 89, 'à': 90, 'é': 91, 'ō': 92, '\\u200d': 93, '–': 94, '‘': 95, '’': 96, '“': 97, '”': 98, '…': 99, '≠': 100}\n",
      "idx to char: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '#', 5: '$', 6: '%', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: '+', 13: ',', 14: '-', 15: '.', 16: '/', 17: '0', 18: '1', 19: '2', 20: '3', 21: '4', 22: '5', 23: '6', 24: '7', 25: '8', 26: '9', 27: ':', 28: ';', 29: '?', 30: 'A', 31: 'B', 32: 'C', 33: 'D', 34: 'E', 35: 'F', 36: 'G', 37: 'H', 38: 'I', 39: 'J', 40: 'K', 41: 'L', 42: 'M', 43: 'N', 44: 'O', 45: 'P', 46: 'Q', 47: 'R', 48: 'S', 49: 'T', 50: 'U', 51: 'V', 52: 'W', 53: 'X', 54: 'Y', 55: 'Z', 56: '[', 57: ']', 58: '_', 59: 'a', 60: 'b', 61: 'c', 62: 'd', 63: 'e', 64: 'f', 65: 'g', 66: 'h', 67: 'i', 68: 'j', 69: 'k', 70: 'l', 71: 'm', 72: 'n', 73: 'o', 74: 'p', 75: 'q', 76: 'r', 77: 's', 78: 't', 79: 'u', 80: 'v', 81: 'w', 82: 'x', 83: 'y', 84: 'z', 85: '{', 86: '|', 87: '}', 88: '~', 89: '\\xa0', 90: 'à', 91: 'é', 92: 'ō', 93: '\\u200d', 94: '–', 95: '‘', 96: '’', 97: '“', 98: '”', 99: '…', 100: '≠'}\n",
      "tokenizing/encoding 'Elon Musk':  [34, 70, 73, 72, 1, 42, 79, 77, 69]\n",
      "detokenizing/decoding it back:  Elon Musk\n"
     ]
    }
   ],
   "source": [
    "char2idx = {char:idx for idx,char in enumerate(vocab)}\n",
    "idx2char = {idx:char for char,idx in char2idx.items()}\n",
    "encode = lambda x: [char2idx[char] for char in x]\n",
    "decode = lambda idxs: \"\".join([idx2char[idx] for idx in idxs])\n",
    "print(\"char to idx:\", char2idx)\n",
    "print(\"idx to char:\", idx2char)\n",
    "print(\"tokenizing/encoding 'Elon Musk': \", encode(\"Elon Musk\"))\n",
    "print(\"detokenizing/decoding it back: \", decode(encode(\"Elon Musk\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the entire corpus in torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded corpus shape: torch.Size([293038]) dtype: torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([38,  1, 64,  ..., 61, 83, 15])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "encoded_corpus = torch.tensor(encode(corpus))\n",
    "print(\"encoded corpus shape:\", encoded_corpus.shape, \"dtype:\", encoded_corpus.dtype)\n",
    "encoded_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a training/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: torch.Size([234430])\n",
      "valid data shape: torch.Size([58608])\n"
     ]
    }
   ],
   "source": [
    "train_split = int(len(encoded_corpus)*0.8)\n",
    "train_data = encoded_corpus[:train_split]\n",
    "valid_data = encoded_corpus[train_split:]\n",
    "print(\"train data shape:\", train_data.shape)\n",
    "print(\"valid data shape:\", valid_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context length == max sequence length == block size  \n",
    "The transformer is trained on each combination of tokens up to the \"context length\".  \n",
    "We are using a context length of 8 so, training would include (0, 1), (0, 1, 2), (0, 1, 2, 3),...,(0, 1, 2, 3, 4, 5, 6, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for index 0: x: I\ty:  \n",
      "for index 1: x: I \ty: f\n",
      "for index 2: x: I f\ty: i\n",
      "for index 3: x: I fi\ty: n\n",
      "for index 4: x: I fin\ty: d\n",
      "for index 5: x: I find\ty:  \n",
      "for index 6: x: I find \ty: t\n",
      "for index 7: x: I find t\ty: h\n"
     ]
    }
   ],
   "source": [
    "context_length = 8\n",
    "for i in range(context_length):\n",
    "    x, y = train_data[:i+1], train_data[i+1]\n",
    "    print(f\"for index {i}: x: {decode(x.tolist())}\\ty: {decode(y[None].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to retrieve a \"batch\" of data from either training or validation datasets. A batch is a sequence of tokens. In a batch, each sub-sequence of tokens must have a target token (the token that comes next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_SEED = 2000\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "shape: torch.Size([4, 8])\n",
      "tensor([[73, 77, 77, 67, 60, 67, 70, 67],\n",
      "        [ 1, 63, 77, 74, 63, 99, 15,  1],\n",
      "        [52, 63, 96, 76, 63,  1, 81, 73],\n",
      "        [67, 63, 62, 15,  1, 33, 73, 72]])\n",
      "targets:\n",
      "shape: torch.Size([4, 8])\n",
      "tensor([[77, 77, 67, 60, 67, 70, 67, 78],\n",
      "        [63, 77, 74, 63, 99, 15,  1, 49],\n",
      "        [63, 96, 76, 63,  1, 81, 73, 76],\n",
      "        [63, 62, 15,  1, 33, 73, 72, 96]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data):\n",
    "    data_len = len(data)\n",
    "    start_idxs = torch.randint(high=data_len - context_length, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in start_idxs])\n",
    "    y = torch.stack([data[i+1: i+context_length+1] for i in start_idxs])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(train_data)\n",
    "print('inputs:')\n",
    "print('shape:',xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print('shape:',yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target value is the token that comes after the input value in the encoded_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
