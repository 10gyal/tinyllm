{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # regex pattern to match mentions\n",
    "    mention_pattern = r'@\\w+'\n",
    "    # regex patterns for emojis and links\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F700-\\U0001F77F\"\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    link_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n",
    "                              flags=re.UNICODE)\n",
    "    \n",
    "    # Remove emojis and links from the tweet\n",
    "    tweet = emoji_pattern.sub('', tweet)\n",
    "    tweet = link_pattern.sub('', tweet)\n",
    "    clean_t = re.sub(mention_pattern, '', tweet)\n",
    "    \n",
    "    return clean_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: clean_tweet(x).strip().replace('&amp;', \"and\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['text'].apply(lambda x: x + '.' if x[-1]!='.' else x).str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character level tokenization is sufficient for a small dataset. First some stats for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 293038\n",
      "Vocab size: 101\n",
      "Vocab content: ['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\xa0', 'à', 'é', 'ō', '\\u200d', '–', '‘', '’', '“', '”', '…', '≠']\n"
     ]
    }
   ],
   "source": [
    "corpus_len = len(corpus)\n",
    "print(\"Corpus length:\", corpus_len)\n",
    "vocab = sorted(list(set(corpus)))\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Vocab content:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a char:index mapping where the index will serve as the index for the token embedding of the char it maps to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char to idx: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, ';': 28, '?': 29, 'A': 30, 'B': 31, 'C': 32, 'D': 33, 'E': 34, 'F': 35, 'G': 36, 'H': 37, 'I': 38, 'J': 39, 'K': 40, 'L': 41, 'M': 42, 'N': 43, 'O': 44, 'P': 45, 'Q': 46, 'R': 47, 'S': 48, 'T': 49, 'U': 50, 'V': 51, 'W': 52, 'X': 53, 'Y': 54, 'Z': 55, '[': 56, ']': 57, '_': 58, 'a': 59, 'b': 60, 'c': 61, 'd': 62, 'e': 63, 'f': 64, 'g': 65, 'h': 66, 'i': 67, 'j': 68, 'k': 69, 'l': 70, 'm': 71, 'n': 72, 'o': 73, 'p': 74, 'q': 75, 'r': 76, 's': 77, 't': 78, 'u': 79, 'v': 80, 'w': 81, 'x': 82, 'y': 83, 'z': 84, '{': 85, '|': 86, '}': 87, '~': 88, '\\xa0': 89, 'à': 90, 'é': 91, 'ō': 92, '\\u200d': 93, '–': 94, '‘': 95, '’': 96, '“': 97, '”': 98, '…': 99, '≠': 100}\n",
      "idx to char: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '#', 5: '$', 6: '%', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: '+', 13: ',', 14: '-', 15: '.', 16: '/', 17: '0', 18: '1', 19: '2', 20: '3', 21: '4', 22: '5', 23: '6', 24: '7', 25: '8', 26: '9', 27: ':', 28: ';', 29: '?', 30: 'A', 31: 'B', 32: 'C', 33: 'D', 34: 'E', 35: 'F', 36: 'G', 37: 'H', 38: 'I', 39: 'J', 40: 'K', 41: 'L', 42: 'M', 43: 'N', 44: 'O', 45: 'P', 46: 'Q', 47: 'R', 48: 'S', 49: 'T', 50: 'U', 51: 'V', 52: 'W', 53: 'X', 54: 'Y', 55: 'Z', 56: '[', 57: ']', 58: '_', 59: 'a', 60: 'b', 61: 'c', 62: 'd', 63: 'e', 64: 'f', 65: 'g', 66: 'h', 67: 'i', 68: 'j', 69: 'k', 70: 'l', 71: 'm', 72: 'n', 73: 'o', 74: 'p', 75: 'q', 76: 'r', 77: 's', 78: 't', 79: 'u', 80: 'v', 81: 'w', 82: 'x', 83: 'y', 84: 'z', 85: '{', 86: '|', 87: '}', 88: '~', 89: '\\xa0', 90: 'à', 91: 'é', 92: 'ō', 93: '\\u200d', 94: '–', 95: '‘', 96: '’', 97: '“', 98: '”', 99: '…', 100: '≠'}\n",
      "tokenizing/encoding 'Elon Musk':  [34, 70, 73, 72, 1, 42, 79, 77, 69]\n",
      "detokenizing/decoding it back:  Elon Musk\n"
     ]
    }
   ],
   "source": [
    "char2idx = {char:idx for idx,char in enumerate(vocab)}\n",
    "idx2char = {idx:char for char,idx in char2idx.items()}\n",
    "encode = lambda x: [char2idx[char] for char in x]\n",
    "decode = lambda idxs: \"\".join([idx2char[idx] for idx in idxs])\n",
    "print(\"char to idx:\", char2idx)\n",
    "print(\"idx to char:\", idx2char)\n",
    "print(\"tokenizing/encoding 'Elon Musk': \", encode(\"Elon Musk\"))\n",
    "print(\"detokenizing/decoding it back: \", decode(encode(\"Elon Musk\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the entire corpus in torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded corpus shape: torch.Size([293038]) dtype: torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([38,  1, 64,  ..., 61, 83, 15])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "encoded_corpus = torch.tensor(encode(corpus))\n",
    "print(\"encoded corpus shape:\", encoded_corpus.shape, \"dtype:\", encoded_corpus.dtype)\n",
    "encoded_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a training/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: torch.Size([234430])\n",
      "valid data shape: torch.Size([58608])\n"
     ]
    }
   ],
   "source": [
    "train_split = int(len(encoded_corpus)*0.8)\n",
    "train_data = encoded_corpus[:train_split]\n",
    "valid_data = encoded_corpus[train_split:]\n",
    "print(\"train data shape:\", train_data.shape)\n",
    "print(\"valid data shape:\", valid_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context length == max sequence length == block size  \n",
    "The transformer is trained on each combination of tokens up to the \"context length\".  \n",
    "We are using a context length of 8 so, training would include (0, 1), (0, 1, 2), (0, 1, 2, 3),...,(0, 1, 2, 3, 4, 5, 6, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for index 0: x: I\ty:  \n",
      "for index 1: x: I \ty: f\n",
      "for index 2: x: I f\ty: i\n",
      "for index 3: x: I fi\ty: n\n",
      "for index 4: x: I fin\ty: d\n",
      "for index 5: x: I find\ty:  \n",
      "for index 6: x: I find \ty: t\n",
      "for index 7: x: I find t\ty: h\n"
     ]
    }
   ],
   "source": [
    "context_length = 8\n",
    "for i in range(context_length):\n",
    "    x, y = train_data[:i+1], train_data[i+1]\n",
    "    print(f\"for index {i}: x: {decode(x.tolist())}\\ty: {decode(y[None].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to retrieve a \"batch\" of data from either training or validation datasets. A batch is a sequence of tokens. In a batch, each sub-sequence of tokens must have a target token (the token that comes next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_SEED = 2000\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "shape: torch.Size([4, 8])\n",
      "tensor([[73, 79, 76,  1, 78, 81, 63, 63],\n",
      "        [67, 72, 69,  1, 67, 78,  1, 81],\n",
      "        [67, 76, 67, 72, 65,  1, 72, 63],\n",
      "        [77,  1, 73, 79, 78,  1, 78, 66]])\n",
      "targets:\n",
      "shape: torch.Size([4, 8])\n",
      "tensor([[79, 76,  1, 78, 81, 63, 63, 78],\n",
      "        [72, 69,  1, 67, 78,  1, 81, 59],\n",
      "        [76, 67, 72, 65,  1, 72, 63, 63],\n",
      "        [ 1, 73, 79, 78,  1, 78, 66, 59]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data):\n",
    "    data_len = len(data)\n",
    "    start_idxs = torch.randint(high=data_len - context_length, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in start_idxs])\n",
    "    y = torch.stack([data[i+1: i+context_length+1] for i in start_idxs])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(train_data)\n",
    "print('inputs:')\n",
    "print('shape:',xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print('shape:',yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target value is the token that comes after the input value in the encoded_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a token, a bigram model predicts the probability of the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(TORCH_SEED)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        logits = self.token_embedding(idx)\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_reshaped = logits.view(B*T, C)\n",
    "            targets_reshaped = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits_reshaped, targets_reshaped)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(idx, targets=None)\n",
    "            logits_last_time_step = logits[:, -1] # Becomes shape (B, C)\n",
    "            # softmax\n",
    "            probs = F.softmax(logits_last_time_step, dim=-1) # Becomes shape (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1) \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out shape: torch.Size([32, 8, 101])\n",
      "xb:  torch.Size([32, 8])\n",
      "yb:  torch.Size([32, 8])\n",
      "loss: tensor(5.1334, grad_fn=<NllLossBackward0>)\n",
      "100 generated tokens: \n",
      "Fsc3nCIK!\n",
      "68~{5l9AUag≠rX}…,u9 0sn‘0UairUM”h6–YN(N \"Ggl’0ZC Pz’D,7KS/f?]“W…J_9f_y≠'.S!x-\n",
      "J6_C4Vl:IQ?‘\n"
     ]
    }
   ],
   "source": [
    "bigram_model = BigramLanguageModel(vocab_size)\n",
    "out, loss = bigram_model(xb, yb)\n",
    "print(\"out shape:\", out.shape)\n",
    "print(\"xb: \", xb.shape)\n",
    "print(\"yb: \", yb.shape)\n",
    "print(\"loss:\", loss)\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(\"100 generated tokens:\", decode(bigram_model.generate(idx, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need an Optimizer for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(bigram_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss 4.974\n",
      "Step 1000: loss 3.959\n",
      "Step 2000: loss 3.255\n",
      "Step 3000: loss 2.952\n",
      "Step 4000: loss 2.687\n",
      "Step 5000: loss 2.620\n",
      "Step 6000: loss 2.516\n",
      "Step 7000: loss 2.643\n",
      "Step 8000: loss 2.650\n",
      "Step 9000: loss 2.463\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    xb, yb = get_batch(train_data)\n",
    "    logits, loss = bigram_model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}: loss {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Havenco ncin r nderf ts, gorge\n"
     ]
    }
   ],
   "source": [
    "print(decode(bigram_model.generate(torch.zeros((1,1), dtype=torch.long),max_new_tokens=30)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this gibberish as a baseline for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss 2.557\n",
      "Step 10: loss 2.606\n",
      "Step 20: loss 2.516\n",
      "Step 30: loss 2.593\n",
      "Step 40: loss 2.584\n",
      "Step 50: loss 2.526\n",
      "Step 60: loss 2.573\n",
      "Step 70: loss 2.584\n",
      "Step 80: loss 2.469\n",
      "Step 90: loss 2.639\n",
      "Step 100: loss 2.681\n",
      "Step 110: loss 2.545\n",
      "Step 120: loss 2.503\n",
      "Step 130: loss 2.502\n",
      "Step 140: loss 2.510\n",
      "Step 150: loss 2.531\n",
      "Step 160: loss 2.476\n",
      "Step 170: loss 2.544\n",
      "Step 180: loss 2.556\n",
      "Step 190: loss 2.405\n",
      "Step 200: loss 2.564\n",
      "Step 210: loss 2.543\n",
      "Step 220: loss 2.701\n",
      "Validation loss: 2.538381576538086\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "losses = []\n",
    "for steps in range(len(valid_data)//batch_size//8):\n",
    "    xb, yb = get_batch(valid_data)\n",
    "    with torch.no_grad():\n",
    "        logits, loss = bigram_model(xb, yb)\n",
    "        losses.append(loss)\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(f\"Step {steps}: loss {loss.item():.3f}\")\n",
    "print(f\"Validation loss: {torch.stack(losses, dim=0).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
